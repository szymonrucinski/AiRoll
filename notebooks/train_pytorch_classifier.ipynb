{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/szymon/.cache/huggingface/datasets/szymonindy___parquet/szymonindy--types-of-film-shots-2e5f392a286cd3ef/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|██████████| 1/1 [00:00<00:00, 248.63it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"szymonindy/types-of-film-shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define a custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.classes = dataset['train'].features['label'].names\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for d in self.dataset[\"train\"]:\n",
    "            self.labels.append(d['label'])\n",
    "            self.images.append(d['image'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        # image = ToTensor()(image).unsqueeze(0)        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = ImageDataset(dataset=dataset, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 740\n",
      "Validation dataset size: 92\n",
      "Test dataset size: 93\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Assuming you have already loaded your dataset into a variable called 'dataset'\n",
    "\n",
    "# Determine the sizes of each split\n",
    "total_size = len(torch_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Use random_split to create the splits\n",
    "train_dataset, val_dataset, test_dataset = random_split(torch_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Verify the sizes of each split\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader\n",
    "batch_size = 32\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "num_classes = len(dataset['train'].features['label'].names)\n",
    "\n",
    "# Replace the last fully connected layer with a new one for the desired number of classes\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training Loss: 1.9477 - Validation Loss: 1.7449\n",
      "Epoch 2/10 - Training Loss: 1.4924 - Validation Loss: 1.4015\n",
      "Epoch 3/10 - Training Loss: 1.0974 - Validation Loss: 1.2098\n",
      "Epoch 4/10 - Training Loss: 0.8450 - Validation Loss: 1.1324\n",
      "Epoch 5/10 - Training Loss: 0.6484 - Validation Loss: 1.0483\n",
      "Epoch 6/10 - Training Loss: 0.4490 - Validation Loss: 1.1476\n",
      "Epoch 7/10 - Training Loss: 0.3093 - Validation Loss: 1.0816\n",
      "Epoch 8/10 - Training Loss: 0.2390 - Validation Loss: 1.0815\n",
      "Epoch 9/10 - Training Loss: 0.1732 - Validation Loss: 1.1124\n",
      "Epoch 10/10 - Training Loss: 0.1375 - Validation Loss: 1.1255\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for images, labels in train_data_loader:\n",
    "        try:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        except:\n",
    "            print(images)\n",
    "            print(labels)\n",
    "            raise\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_data_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += val_loss.item()\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(valid_data_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {epoch_loss:.4f} - Validation Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Precision: 0.5295\n",
      "Recall: 0.5301\n",
      "F1 Score: 0.5188\n",
      "Accuracy: 0.5591\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mismatched Tensor types in NNPack convolutionOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m# Select a random image and target class from the test dataset\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m image, label \u001b[39min\u001b[39;00m test_data_loader:\n\u001b[1;32m     67\u001b[0m     \u001b[39m# Generate the heat map for the selected image and target class\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     heat_map \u001b[39m=\u001b[39m generate_gradcam(image, label)\n\u001b[1;32m     70\u001b[0m     \u001b[39m# Convert the heat map to a color map\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     color_map \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mapplyColorMap(np\u001b[39m.\u001b[39muint8(\u001b[39m255\u001b[39m \u001b[39m*\u001b[39m heat_map), cv2\u001b[39m.\u001b[39mCOLORMAP_JET)\n",
      "Cell \u001b[0;32mIn[136], line 31\u001b[0m, in \u001b[0;36mgenerate_gradcam\u001b[0;34m(img, target_class)\u001b[0m\n\u001b[1;32m     28\u001b[0m img\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[1;32m     30\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     32\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Check if the predicted class matches the target class\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/airoll/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/airoll/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/airoll/lib/python3.9/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/airoll/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/airoll/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/airoll/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatched Tensor types in NNPack convolutionOutput"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already loaded your test dataset into a variable called 'test_dataset'\n",
    "# Assuming you have already defined your model as 'model'\n",
    "\n",
    "# Preprocess the images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Grad-CAM function\n",
    "def generate_gradcam(img, target_class):\n",
    "    # Convert the image to a tensor and apply preprocessing\n",
    "    # img = preprocess(img).unsqueeze(0)\n",
    "    print(img.shape)\n",
    "    \n",
    "    # Enable gradient calculation\n",
    "    img.requires_grad_()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(img)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Check if the predicted class matches the target class\n",
    "    if predicted.item() == target_class:\n",
    "        # Calculate gradients\n",
    "        model.zero_grad()\n",
    "        one_hot = torch.zeros_like(outputs)\n",
    "        one_hot[0][target_class] = 1\n",
    "        outputs.backward(gradient=one_hot)\n",
    "        \n",
    "        # Get the gradients of the output with respect to the feature maps\n",
    "        grads = img.grad[0].detach().cpu().numpy()\n",
    "        \n",
    "        # Get the feature maps\n",
    "        feature_maps = model.feature_maps[0].detach().cpu().numpy()\n",
    "        \n",
    "        # Calculate the weights\n",
    "        weights = np.mean(grads, axis=(1, 2))\n",
    "        \n",
    "        # Generate the heat map\n",
    "        heat_map = np.zeros_like(feature_maps[0])\n",
    "        for i, w in enumerate(weights):\n",
    "            heat_map += w * feature_maps[i]\n",
    "        \n",
    "        # Normalize the heat map\n",
    "        heat_map = np.maximum(heat_map, 0)\n",
    "        heat_map /= np.max(heat_map)\n",
    "        \n",
    "        # Resize the heat map to match the original image size\n",
    "        heat_map = cv2.resize(heat_map, (img.size(3), img.size(2)))\n",
    "        \n",
    "        return heat_map\n",
    "\n",
    "# Select a random image and target class from the test dataset\n",
    "for image, label in test_data_loader:\n",
    "    # Generate the heat map for the selected image and target class\n",
    "    for i in range(32):\n",
    "        heat_map = generate_gradcam(image, label)\n",
    "\n",
    "    # Convert the heat map to a color map\n",
    "    color_map = cv2.applyColorMap(np.uint8(255 * heat_map), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Overlay the color map on the original image\n",
    "    overlay_img = cv2.addWeighted(np.uint8(255 * image.permute(1, 2, 0)), 0.5, color_map, 0.5, 0)\n",
    "\n",
    "    # Plot the original image, heat map, and overlaid image\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(image.permute(1, 2, 0))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(heat_map, cmap=\"jet\")\n",
    "    axes[1].set_title(\"Heat Map\")\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[2].imshow(overlay_img)\n",
    "    axes[2].set_title(\"Overlay\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airoll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
