{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/szymon/Documents/AiRoll/notebooks/a_finetune_resnet50.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/szymon/Documents/AiRoll/notebooks/a_finetune_resnet50.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m ToTensor\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/szymon/Documents/AiRoll/notebooks/a_finetune_resnet50.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/szymon/Documents/AiRoll/notebooks/a_finetune_resnet50.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/szymon/Documents/AiRoll/notebooks/a_finetune_resnet50.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"szymonindy/types-of-film-shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][4][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(dataset[\"train\"][17][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define a custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.classes = dataset['train'].features['label'].names\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for d in self.dataset[\"train\"]:\n",
    "            self.labels.append(d['label'])\n",
    "            self.images.append(d['image'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        # image = ToTensor()(image).unsqueeze(0)        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = ImageDataset(dataset=dataset, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Assuming you have already loaded your dataset into a variable called 'dataset'\n",
    "\n",
    "# Determine the sizes of each split\n",
    "total_size = len(torch_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Use random_split to create the splits\n",
    "train_dataset, val_dataset, test_dataset = random_split(torch_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Verify the sizes of each split\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader\n",
    "batch_size = 32\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "num_classes = len(dataset['train'].features['label'].names)\n",
    "\n",
    "# Replace the last fully connected layer with a new one for the desired number of classes\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for images, labels in train_data_loader:\n",
    "        try:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        except:\n",
    "            print(images)\n",
    "            print(labels)\n",
    "            raise\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_data_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += val_loss.item()\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(valid_data_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {epoch_loss:.4f} - Validation Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already defined your model as 'model'\n",
    "# Assuming you have already loaded your test data loader into a variable called 'test_data_loader'\n",
    "\n",
    "# Preprocess the images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Grad-CAM function\n",
    "def generate_gradcam(img, target_class):\n",
    "    # Convert the image to a tensor and apply preprocessing\n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = preprocess(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    # Enable gradient calculation\n",
    "    img.requires_grad_()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(img)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Check if the predicted class matches the target class\n",
    "    if predicted.item() == target_class:\n",
    "        # Calculate gradients\n",
    "        model.zero_grad()\n",
    "        one_hot = torch.zeros_like(outputs)\n",
    "        one_hot[0][target_class] = 1\n",
    "        outputs.backward(gradient=one_hot)\n",
    "        \n",
    "        # Get the gradients of the output with respect to the feature maps\n",
    "        grads = img.grad[0].detach().cpu().numpy()\n",
    "        \n",
    "        # Get the feature maps\n",
    "        feature_maps = model.feature_maps[0].detach().cpu().numpy()\n",
    "        \n",
    "        # Calculate the weights\n",
    "        weights = np.mean(grads, axis=(1, 2))\n",
    "        \n",
    "        # Generate the heat map\n",
    "        heat_map = np.zeros_like(feature_maps[0])\n",
    "        for i, w in enumerate(weights):\n",
    "            heat_map += w * feature_maps[i]\n",
    "        \n",
    "        # Normalize the heat map\n",
    "        heat_map = np.maximum(heat_map, 0)\n",
    "        heat_map /= np.max(heat_map)\n",
    "        \n",
    "        # Resize the heat map to match the original image size\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            img = transforms.functional.to_pil_image(img)\n",
    "        heat_map = cv2.resize(heat_map, (img.size[0], img.size[1]))\n",
    "        \n",
    "        return heat_map\n",
    "\n",
    "# Select a random image and label from the test data loader\n",
    "index = np.random.randint(len(test_data_loader.dataset))\n",
    "image, label = test_data_loader.dataset[index]\n",
    "\n",
    "# Generate the heat map for the selected image and label\n",
    "heat_map = generate_gradcam(image, label)\n",
    "\n",
    "# Convert the heat map to a color map\n",
    "color_map = cv2.applyColorMap(np.uint8(255 * heat_map), cv2.COLORMAP_JET)\n",
    "\n",
    "# Overlay the color map on the original image\n",
    "overlay_img = cv2.addWeighted(np.uint8(255 * transforms.functional.to_pil_image(image)), 0.5, color_map, 0.5, 0)\n",
    "\n",
    "# Plot the original image, heat map, and overlaid image\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(heat_map, cmap=\"jet\")\n",
    "axes[1].set_title(\"Heat Map\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(overlay_img)\n",
    "axes[2].set_title(\"Overlay\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airoll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
